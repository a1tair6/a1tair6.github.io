# Hadoop 3.1.2 설치

- hadoop 3.1.2, hive 3.1.1, spark 2.4.2, tez 0.9.2, zookeeper 3.4.14
- instance aws ec2 ami
- name node 2ea
- data node 3ea
- type m5.xlarge

## Instance Setting
### java
#### 설치
`apt-get install openjdk-8-jdk`

#### 확인
`java -version`

### 계정 생성
`sudo adduser hadoop`
#### 비밀번호 설정
`passwd hadoop`

### ssh
#### 설치
`apt-get install openssh-server`

- public key 생성
`ssh-keygen -t rsa -P`

- 인증키로 등록
`cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys`

- ssh 접속 확인
`ssh localhost`

- 전체 node에 namenode 2ea의 public key 등록
`vi ~/.ssh/authorized_keys` (copy & paste or rsync)

### host 파일 등록
```
X.X.X.X host-nn-01
X.X.X.X host-nn-02
X.X.X.X host-dn-01
X.X.X.X host-dn-02
X.X.X.X host-dn-03
```

### .bashrc config
```sh
export HADOOP_HOME="/home/hadoop/eco_service/hadoop"
export HADOOP_CONF_DIR="/home/hadoop/eco_service/hadoop/etc/hadoop"
export HIVE_HOME="/home/hadoop/eco_service/hive"
export SPARK_HOME="/home/hadoop/eco_service/spark"
export LD_LIBRARY_PATH="$HADOOP_HOME/lib/native"
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$SPARK_HOME/bin:$LD_LIBRARY_PATH
export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH
```

## zookeeper install
### zookeeper 3.4.14 binary download
```sh
cd /home/hadoop/installed
wget http://apache.tt.co.kr/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz
tar -xvf zookeeper-3.4.14.tar.gz
rm -rf zookeeper-3.4.14.tar.gz
ln -s /home/hadoop/installed/zookeeper-3.4.14 /home/hadoop/eco_service/zookeeper
```

### zookeeper config
```sh
cp /home/hadoop/eco_service/zookeeper/conf/zoo_sample.cfg /home/hadoop/eco_service/zookeeper/conf/zoo.cfg
vi /home/hadoop/eco_service/zookeeper/conf/zoo.cfg
```

### zoo.conf
```
# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just
# example sakes.
dataDir=/home/hadoop/eco_service/zookeeper/data
# the port at which the clients will connect
clientPort=2181
# the maximum number of client connections.
# increase this if you need to handle more clients
maxClientCnxns=60
maxSessionTimeout=180000
#
# Be sure to read the maintenance section of the
# administrator guide before turning on autopurge.
#
# http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
#
# The number of snapshots to retain in dataDir
#autopurge.snapRetainCount=3
# Purge task interval in hours
# Set to "0" to disable auto purge feature
#autopurge.purgeInterval=1
server.1=host-nn-001-01:2888:3888
server.2=host-nn-001-02:2888:3888
server.3=host-dn-001-01:2888:3888
```

### create zookeeper myid
```sh
namenode 1번
echo 1 > /home/hadoop/eco_service/zookeeper/data/myid
namenode 2번
echo 2 > /home/hadoop/eco_service/zookeeper/data/myid
datanode 1번
echo 3 > /home/hadoop/eco_service/zookeeper/data/myid
```

### run zookeeper
`home/hadoop/eco_service/zookeeper/bin/zkServer.sh start`


## hadoop install
- workdir은 /home/hadoop
- 설치된 package /home/hadoop/installed 밑에
- soft link 로 /home/hadoop/eco_service 밑에

### hadoop 3.1.2 binary download
```sh
cd /home/hadoop/installed
wget http://apache.tt.co.kr/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz
tar -xvf hadoop-3.1.2.tar.gz
rm -rf hadoop-3.1.2.tar.gz
ln -s /home/hadoop/installed/hadoop-3.1.2 /home/hadoop/eco_service/hadoop
```


### hadoop config

#### start journalnode
`bin/hadoop-daemon.sh start journalnode`

#### format namenode
`hdfs namenode -format`

#### start hdfs(name, resourcemanager, datanode)
`sbin/start-all.sh`


## hive install
- apache hive-3.1.2 version

### hive 3.1.2 binary download
```sh
cd /home/hadoop/installed
wget http://apache.tt.co.kr/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
tar -xvf apache-hive-3.1.2-bin.tar.gz
rm -rf apache-hive-3.1.2-bin.tar.gz
ln -s /home/hadoop/installed/apache-hive-3.1.2-bin /home/hadoop/eco_service/hive
```

### hive config
```sh
cp /home/hadoop/eco_service/hive/conf/hive-env.sh.template /home/hadoop/eco_service/hive/conf/hive-env.sh
cp /home/hadoop/eco_service/hive/conf/hive-default.xml.template /home/hadoop/eco_service/hive/conf/hive-site.xml
```

### start metastore, hiveserver2
```sh
bin/hive service metastore &
bin/hive service hiveserver2 &
```

## tez install
### apache-tez 0.9.2 binary download
```sh
cd /home/hadoop/installed
wget http://apache.tt.co.kr/tez/0.9.2/apache-tez-0.9.2-bin.tar.gz
tar -xvf apache-tez-0.9.2-bin.tar.gz
rm -rf apache-tez-0.9.2-bin.tar.gz
ln -s /home/hadoop/installed/apache-tez-0.9.2-bin.tar.gz /home/hadoop/eco_service/tez
```

### tez config
```sh
vi /home/hadoop/eco_service/tez/tez-site.xml
```

### upload tez tarball, tez-site.xml
```sh
hadoop fs -mkdir /apps
cd /home/hadoop/installed
hadoop fs -put tez-0.9.2 /apps
```

## spark install
### spark 2.4.4 binary download
```sh
cd /home/hadoop/installed
wget http://apache.tt.co.kr/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
tar -xvf spark-2.4.4-bin-hadoop2.7.tgz
rm -rf spark-2.4.4-bin-hadoop2.7.tgz
ln -s /home/hadoop/installed/spark-2.4.4-bin-hadoop2.7 /home/hadoop/eco_service/spark
```

### spark config
#### mysql connector download
```sh
cd /home/hadoop/eco_service/spark/jars
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar
```
